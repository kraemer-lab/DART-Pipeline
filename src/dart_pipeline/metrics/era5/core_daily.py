"""Core ERA5 processing for daily zonal aggregation"""

import logging
import functools
import multiprocessing
from typing import Literal
from pathlib import Path
from functools import cache

import xarray as xr

from geoglue.util import sha256, set_lonlat_attrs
from geoglue.region import gadm
from geoglue.cds import CdsDataset
from geoglue.resample import resample

from ...metrics import (
    register_process,
    zonal_stats_xarray,
)
from ...metrics.worldpop import get_worldpop
from ...util import iso3_admin_unpack
from ...paths import get_path

from .util import (
    get_dataset_pool,
    add_bias_corrected_tp,
    specific_humidity,
    relative_humidity,
)
from .list_metrics import (
    VARIABLE_MAPPINGS,
    METRICS,
    ACCUM_METRICS,
    INSTANT_METRICS,
    DERIVED_METRICS_SEPARATE_IMPL,
)
from .util import pprint_ms

logger = logging.getLogger(__name__)

STATS = ["min", "mean", "max", "sum"]


@cache
def get_resampled_paths_daily(iso3: str, year: int) -> dict[str, Path]:
    return {
        stat: get_path(
            "scratch", iso3, "era5", f"{iso3}-{year}-era5.daily_{stat}.resampled.nc"
        )
        for stat in ["mean", "min", "max", "sum"]
    }


def population_weighted_aggregation_daily(
    metric: str,
    statistic: str,
    iso3: str,
    admin: int,
    year: int,
) -> Path:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(processName)s] %(levelname)s %(message)s",
    )
    resampled_paths = get_resampled_paths_daily(iso3, year)
    message = f"zonal stats [{iso3}-{admin}, {year}] era5.{metric}.daily_{statistic}"

    logger = logging.getLogger(__name__)
    logger.info(f"START {message}")
    operation = (
        "mean(coverage_weight=area_spherical_km2)"
        if metric not in ACCUM_METRICS
        else "area_weighted_sum"
    )
    variable = VARIABLE_MAPPINGS.get(metric, metric)
    resampled_checksum = sha256(resampled_paths[statistic])
    ds = xr.open_dataset(resampled_paths[statistic])
    provenance = ds.attrs.get("provenance", "")
    da = zonal_stats_xarray(
        f"era5.{metric}.daily_{statistic}",
        ds[variable],
        gadm(iso3, admin),
        operation=operation,
        weights=get_worldpop(iso3, year),
    )
    # clamp relative_humidity to 100%
    if "relative_humidity" in metric:
        da = da.clip(0, 100)
    outfile = metric_path_daily(iso3, admin, year, metric, statistic)
    assert outfile.suffix == ".nc"
    da.attrs["provenance"] = (
        f"zonal_stats.ds={resampled_checksum} {resampled_paths[statistic]}\n{provenance}"
    )
    da.to_netcdf(outfile)
    logger.info(f"  END {message} -> {outfile}")
    return outfile


def collect_variables_to_drop(kind: Literal["instant", "accum"]) -> list[str]:
    "Collect list of variables to drop for a particular variable type"
    ms = INSTANT_METRICS if kind == "instant" else ACCUM_METRICS
    collect = set()
    for m in ms:
        collect.update(METRICS[m].get("depends", []))
    vars_to_drop = sorted(collect - set(METRICS.keys()))
    return [VARIABLE_MAPPINGS.get(v, v) for v in vars_to_drop]


def metric_path_daily(
    iso3: str, admin: int, year: int, metric: str, statistic: str
) -> Path:
    assert statistic in STATS
    return get_path(
        "output",
        iso3,
        "era5",
        f"{iso3}-{admin}-{year}-era5.{metric}.daily_{statistic}.nc",
    )


@register_process("era5.core")
def era5_process_core_daily(
    iso3: str, date: str, overwrite: bool = True, keep_resampled: bool = False
) -> list[Path]:
    """Processes ERA5 data for a particular year

    Parameters
    ----------
    iso3 : str
        Country ISO 3166-2 alpha-3 code
    date : str
        Year for which to process ERA5 data
    overwrite : bool
        Whether to overwrite existing generated data, default=True
    keep_resampled : bool
        Whether to keep files generated by CDO resample. These can be large files
        due to upsampling of source data. Default is False, and resampled files
        are deleted upon completion of processing.

    Returns
    -------
    List of generated or pre-existing data files in parquet format
    """
    year = int(date)
    iso3, admin = iso3_admin_unpack(iso3)
    logger.info(f"Processing {iso3}-{admin}-{year}-era5.core")
    paths = {
        stat: get_path("scratch", iso3, "era5", f"{iso3}-{year}-era5.daily_{stat}.nc")
        for stat in ["mean", "min", "max", "sum"]
    }
    # after cdo resampling
    resampled_paths = get_resampled_paths_daily(iso3, year)

    iso3 = iso3.upper()
    pool = get_dataset_pool(iso3)
    ds = pool[year]

    # calculate derived metrics
    ds.instant["q"] = specific_humidity(ds.instant)
    ds.instant["r"] = relative_humidity(ds.instant)
    ds.accum["hb"] = ds.accum.tp + ds.accum.e

    # calculate relative humidity

    ds = CdsDataset(
        instant=ds.instant.drop_vars(collect_variables_to_drop("instant")),
        accum=ds.accum.drop_vars(collect_variables_to_drop("accum")),
    )

    logger.info("Calculating daily statistics (mean, sum)")
    daily_agg = ds.daily()  # mean and sum
    daily_agg.instant.to_netcdf(paths["mean"])

    # Read in possible tp_corrected file here and add to accum dataset
    # If no tp_corrected file is found, add_bias_corrected_tp() returns
    # the daily accumulated dataset unaltered.
    # Note that tp_corrected for a year will require the corresponding files
    # for previous and succeeding years depending on shift_hours
    accum = add_bias_corrected_tp(daily_agg.accum, iso3, year)
    is_bias_corrected: bool = "tp_bc" in accum.variables
    if is_bias_corrected:
        accum["hb_bc"] = accum.tp_bc + accum.e
    set_lonlat_attrs(accum)
    accum.to_netcdf(paths["sum"])

    # read in
    logger.info("Calculating daily statistics (min, max)")
    ds.daily_max().to_netcdf(paths["max"])
    ds.daily_min().to_netcdf(paths["min"])

    instant_metrics = [
        m for m in INSTANT_METRICS if m not in DERIVED_METRICS_SEPARATE_IMPL
    ]
    accum_metrics = [m for m in ACCUM_METRICS if m not in DERIVED_METRICS_SEPARATE_IMPL]
    if not is_bias_corrected:
        instant_metrics = [m for m in instant_metrics if not m.endswith("_corrected")]
        accum_metrics = [m for m in accum_metrics if not m.endswith("_corrected")]

    metric_statistic_combinations: dict[str, list[str]] = {
        s: instant_metrics for s in ["min", "max", "mean"]
    }
    metric_statistic_combinations = {"sum": accum_metrics}

    already_existing_metrics: dict[str, list[str]] = {
        s: [
            m
            for m in metric_statistic_combinations[s]
            if metric_path_daily(iso3, admin, year, m, s).exists()
        ]
        for s in metric_statistic_combinations
    }
    n_already_existing_metrics: int = sum(
        len(already_existing_metrics[s]) for s in already_existing_metrics
    )

    generated_paths = []
    if not overwrite and n_already_existing_metrics:
        generated_paths = sum(
            [
                [
                    metric_path_daily(iso3, admin, year, m, s)
                    for m in already_existing_metrics[s]
                ]
                for s in already_existing_metrics
            ],
            [],
        )
        logger.info(
            "Metric statistic combinations: %s",
            pprint_ms(metric_statistic_combinations, already_existing_metrics),
        )
        # filter to keep only metrics that need to be calculated
        metric_statistic_combinations = {
            s: [
                m
                for m in metric_statistic_combinations[s]
                if m not in already_existing_metrics[s]
            ]
            for s in metric_statistic_combinations
        }
    else:
        logger.info(
            "Metric statistic combinations: %s",
            pprint_ms(metric_statistic_combinations),
        )

    for stat in metric_statistic_combinations:
        # skip if no metrics are requested to be generated for statistic
        metrics = metric_statistic_combinations[stat]
        if not metrics:
            continue
        logger.info("Computing zonal aggregation for statistic=%s", stat)
        resampling = "remapdis" if stat == "sum" else "remapbil"
        logger.info(
            f"Resampling using CDO for {stat=} using {resampling=}: {paths[stat]} -> {resampled_paths[stat]}"
        )
        resample(
            resampling, paths[stat], get_worldpop(iso3, year), resampled_paths[stat]
        )
        with multiprocessing.Pool() as p:
            new_paths = list(
                p.map(
                    functools.partial(
                        population_weighted_aggregation_daily,
                        statistic=stat,
                        iso3=iso3,
                        admin=admin,
                        year=year,
                    ),
                    metrics,
                )
            )
        if not keep_resampled:
            resampled_paths[stat].unlink()
        generated_paths += new_paths

    return generated_paths
